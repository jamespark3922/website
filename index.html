<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!--<script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>-->
  <!--<script>-->
    <!--window.dataLayer = window.dataLayer || [];-->
    <!--function gtag(){dataLayer.push(arguments);}-->
    <!--gtag('js', new Date());-->

    <!--gtag('config', 'UA-7580334-2');-->
  <!--</script>-->

  <title>Jae Sung Park</title>

  <meta name="author" content="Jae Sung Park">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/allenschool.png">
</head>

<body>
  <table style="width:100%;max-width:1100px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px"> <!-- page row -->
      <td style="padding:0px">  <!-- page column -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jae Sung (James) Park</name>
              </p>
            
              <div id="description">
                <p>I am a final-year PhD student at University of Washington in <a href="https://www.cs.washington.edu/" target="_blank"> Computer Science and Engineering </a> advised by <a href="https://homes.cs.washington.edu/~yejin/" target="_blank"> Yejin Choi</a>, <a href="https://homes.cs.washington.edu/~ali/" target="_blank"> Ali Farhadi</a>, and <a href="https://www.ranjaykrishna.com/index.html" target="_blank"> Ranjay Krishna</a>.
                  Previously, I received my B.S. degree in EECS at University of California, Berkeley, where I worked closely with <a href="https://anna-rohrbach.net/" target="_blank"> Anna Rohrbach </a> and <a href="https://people.eecs.berkeley.edu/~trevor/" target="_blank"> Trevor Darrell </a>.
                </p>
              </div>

              <div id="interests">
                <p>
                  I am interested in how machines use <b>visual perception</b> and <b>language understanding to reason about the visual world </b> in a way humans do. 
                  Specifically, my research projects have been focused on: 
                  <ul>
                    <li>Empowering <b>Visual Commonsense Reasoning</b> of AI models</li>
                    <li><b>Grounding</b> Objects, Concepts, Actions to Images and Videos</li>
                    <li><b>Evaluation</b> of Multimodal Language Models</li>
                  </ul>
                </p>
            </div>

            <div id="info">
              <p style="text-align:center">
                <a href="mailto:jspark96@cs.washington.edu" target="_blank">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=hD2WqqcAAAAJ&hl=en" target="_blank">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/jamespark3922" target="_blank">Github</a> &nbsp &nbsp 
              </p>
            </div>
            </td>

            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="https://raw.githubusercontent.com/jamespark3922/website/refs/heads/master/images/profile/james.png" target="_blank"><img style="width:80%;max-width:80%" alt="profile photo" src="https://storage.googleapis.com/ai2-jamesp-public/personal-website/james.png" class="hoverZoomLink"></a>
            </td>

          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tbody><tr><td>
            <heading>News</heading>
            <ul>
              <li>[12/2024] 1 paper at Neurips; 1 paper at Neurips D&B.</li>
              <li>[09/2024] Released <a href="https://molmo.allenai.org/blog" target="_blank"><strong>Molmo</strong></a>, an <span class="emph">open</span> state-of-the-art multimodal AI model [<a href="https://molmo.allenai.org/" target="_blank">demo</a>] [<a href="https://github.com/allenai/molmo" target="_blank">code</a>]</li>
              <li>[09/2024] Co-organized <a href="https://multimodalagents.github.io/" target="_blank">ECCV2024 Workshop on Multimodal Agents</a>.</li>
              <li>[06/2024] Co-oragnized <a href="https://multimodalagentai.github.io/" target="_blank">CVPR 2024 Tutorial on Generalist Agent AI</a>.</li>
              <li>[03/2024] Internship @ <a href="https://www.microsoft.com/en-us/research/group/deep-learning-group/" target="_blank"> Microsoft Research Deep Learning Team</a> in Spring 2022 - Winter 2024.</li> 

              <!-- <div style="visibility:hidden;"> 
                <li>Internship @ <a href="https://www.amazon.science/"> Amazon Alexa AI Team</a> in Winter 2022.</li>
                <li>Internship @ <a href="https://mosaic.allenai.org"> AI2 Mosaic Team </a> in Winter 2020.</li>
                <li>Co-organized <a href="https://sites.google.com/site/describingmovies/lsmdc-2019"> Large Scale Movie Description Challenge 2019 </a> appeared in <a href="https://sites.google.com/site/iccv19clvllsmdc/program"> ICCV19 CLVL workshop</a>. </li>
                <li>Started PhD at University of Washington in Fall 2019 - Present.</li> 
              </div> -->

            </ul>
          </td></tr>
        </tbody></table>
        

        <div id="research"> <!-- Research -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10" ><tbody>
          <tr>
            <td>
              <heading>Research</heading>
            </td>
          </tr>

          <tr>
            <td width="80%" valign="center" style="padding-left: 3%;">
              <a href="https://arxiv.org/abs/2409.17146" target="_blank">
                <papertitle>Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models.</papertitle>
              </a>
              <br>
              Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, <strong>Jae Sung Park</strong>, Mohammadreza Salehi ... Ranjay Krishna, Luca Weihs, Noah A Smith, Hannaneh Hajishirzi, <br> Ross Girshick, Ali Farhadi, Aniruddha Kembhavi
              <br>
              <em>arxiv</em>, 2024 <strong></strong>
              <br>
              <a href="https://arxiv.org/abs/2409.17146" target="_blank">arXiv</a> / 
              <a href="https://molmo.allenai.org/" target="_blank">demo</a> /
              <a href="https://huggingface.co/collections/allenai/pixmo-674746ea613028006285687b" target="_blank">dataset</a> /
              <a href="https://github.com/allenai/molmo" target="_blank">code</a>
            </td>
          </tr>

          <tr>
            <td width="80%" valign="center" style="padding-left: 3%;">
              <a href="https://arxiv.org/abs/2411.07461" target="_blank">
                <papertitle>BLIP3-KALE: Knowledge Augmented Large-Scale Dense Caption</papertitle>
              </a>
              <br>
              Anas Awadalla, Le Xue, Manli Shu, An Yan, Jun Wang, Senthil Purushwalkam, Sheng Shen, Hannah Lee, Oscar Lo, <strong>Jae Sung Park</strong>, Etash Guha, Silvio Savarese, Ludwig Schmidt, Yejin Choi, Caiming Xiong, Ran Xu
              <br>
              <em>arxiv</em>, 2024 <strong></strong>
              <br>
              <a href="https://arxiv.org/abs/2411.07461" target="_blank">arXiv</a>  /
              <a href="https://huggingface.co/datasets/Salesforce/blip3-kale" target="_blank">dataset</a>
            </td>
          </tr>

          <tr>
            <td width="80%" valign="center" style="padding-left: 3%;">
              <a href="https://arxiv.org/abs/2407.01942" target="_blank">
                <papertitle>Certainly Uncertain: A Benchmark and Metric for Multimodal Epistemic and Aleatoric Awareness.</papertitle>
              </a>
              <br>
              Khyathi Raghavi Chandu, Linjie Li, Anas Awadalla, Ximing Lu, <strong>Jae Sung Park</strong>, Jack Hessel, Lijuan Wang, Yejin Choi
              <br>
              <em>arxiv</em>, 2024 <strong></strong>
              <br>
              <a href="https://arxiv.org/abs/2407.01942" target="_blank">arXiv</a> 
            </td>
          </tr>

          <tr>
            <td width="80%" valign="center" style="padding-left: 3%;">
              <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf" target="_blank">
                <papertitle>ActionAtlas: A VideoQA Benchmark for Domain-specialized Action Recognition</papertitle>
              </a>
              <br>
              Mohammadreza Salehi, <strong>Jae Sung Park</strong>, Tanush Yadav, Aditya Kusupati, Ranjay Krishna, Yejin Choi, Hannaneh Hajishirzi, Ali Farhad
              <br>
              <em>Neurips Dataset & Benchmarks</em>, 2024 <strong></strong>
              <br>
              <a href="https://arxiv.org/pdf/2410.05774" target="_blank">arXiv</a> /
              <a href="https://mrsalehi.github.io/action-atlas/" target="_blank">website</a>
              
            </td>
          </tr>

          <tr>
            <td width="80%" valign="center" style="padding-left: 3%;">
              <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf" target="_blank">
                <papertitle>Superposed Decoding: Multiple Generations from a Single Autoregressive Inference Pass</papertitle>
              </a>
              <br>
              Ethan Shen, Alan Fan, Sarah Pratt, <strong>Jae Sung Park</strong>, Matthew Wallingford, Sham Kakade, Ari Holtzman, Ranjay Krishna, Ali Farhadi, Aditya Kusupati.
              <br>
              <em>Neurips</em>, 2024 <strong></strong>
              <br>
              <a href="https://arxiv.org/abs/2405.18400" target="_blank">arXiv</a>
            </td>
          </tr>

          <tr>
            <td width="80%" valign="center" style="padding-left: 3%;">
              <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/257be12f31dfa7cc158dda99822c6fd1-Paper-Conference.pdf" target="_blank">
                <papertitle>Localized Symbolic Knowledge Distillation for Visual Commonsense Models</papertitle>
              </a>
              <br>
              <strong>Jae Sung Park</strong>, Jack Hessel, Khyathi Chandu, Paul Pu Liang, Ximing Lu, Peter West,
              Youngjae Yu, Qiuyuan Huang, Jianfeng Gao, Ali Farhadi, Yejin Choi
              <br>
              <em>Neurips</em>, 2023 <strong></strong>
              <br>
              <a href="https://arxiv.org/abs/2312.04837" target="_blank">arXiv</a>
            </td>
          </tr>

          <tr>
            <td width="80%" valign="center" style="padding-left: 3%;">
              <a href="https://arxiv.org/pdf/2205.12630.pdf" target="_blank">
                <papertitle>Multimodal knowledge alignment with reinforcement learning</papertitle>
              </a>
              <br>
              Youngjae Yu, Jiwan Chung, Heeseung Yun, Jack Hessel, <strong>Jae Sung Park</strong>, Ximing Lu, Prithviraj Ammanabrolu, Rowan Zellers, Ronan Le Bras, Gunhee Kim, Yejin Choi
              <br>
              <em>CVPR</em>, 2023 <strong></strong>
              <br>
              <a href="https://arxiv.org/pdf/2205.12630.pdf" target="_blank">arXiv</a>
            </td>
          </tr>

          <tr>
            <td width="80%" valign="center" style="padding-left: 3%;">
              <a href="https://aclanthology.org/2022.naacl-main.261.pdf" target="_blank">
                <papertitle>Exposing the limits of video-text models through contrast sets</papertitle>
              </a>
              <br>
              <strong>Jae Sung Park</strong>, Sheng Shen, Ali Farhadi, Trevor Darrell, Yejin Choi, Anna Rohrbach
              <br>
              <em>NAACL (short)</em>, 2022 <strong></strong>
              <br>
              <a href="https://aclanthology.org/2022.naacl-main.261.pdf" target="_blank">arXiv</a> /
              <a href="https://github.com/jamespark3922/video-lang-contrast-set" target="_blank">code</a>
            </td>
          </tr>

          <tr>
            <td width="80%" valign="center" style="padding-left: 3%;">
              <a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/c6d4eb15f1e84a36eff58eca3627c82e-Paper.pdf" target="_blank">
                <papertitle>Merlot: Multimodal neural script knowledge models</papertitle>
              </a>
              <br>
              Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, <strong>Jae Sung Park</strong>, Jize Cao, Ali Farhadi, Yejin Choi
              <br>
              <em>Neurips</em>, 2021 <strong></strong>
              <br>
              <a href="https://arxiv.org/abs/2106.02636" target="_blank">arXiv</a>
            </td>
          </tr>

          <tr>
            <td width="80%" valign="center" style="padding-left: 3%;">
              <a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/c88d8d0a6097754525e02c2246d8d27f-Paper.pdf" target="_blank">
                <papertitle>LLC: Accurate, multi-purpose learnt low-dimensional binary codes</papertitle>
              </a>
              <br>
              Aditya Kusupati, Matthew Wallingford, Vivek Ramanujan, Raghav Somani, <strong>Jae Sung Park</strong>, Krishna Pillutla, Prateek Jain, Sham Kakade, Ali Farhadi
              <br>
              <em>Neurips</em>, 2021 <strong></strong>
              <br>
              <a href="https://arxiv.org/abs/2106.01487" target="_blank">arXiv</a>
            </td>
          </tr>

          <tr>
            <!-- <td width="40%" valign="center" align="center"><img src="images/papers/visualcomet20.jpg" alt="sym" width="100%" style="border-radius:15px"></a></td> -->
            <td width="80%" valign="center" style="padding-left: 3%;">
              <a href="https://arxiv.org/pdf/2010.07526" target="_blank">
                <papertitle>Natural language rationales with full-stack visual reasoning: From pixels to semantic frames to commonsense graphs</papertitle>
              </a>
              <br>
              Ana Marasović, Chandra Bhagavatula, <strong>Jae Sung Park</strong>, Ronan Le Bras, Noah A Smith, Yejin Choi
              <br>
              <em>Findings of EMNLP</em>, 2020 <strong></strong>
              <br>
              <a href="https://arxiv.org/pdf/2010.07526" target="_blank">arXiv</a>
            </td>
          </tr>

          <tr>
            <!-- <td width="40%" valign="center" align="center"><img src="images/papers/visualcomet20.jpg" alt="sym" width="100%" style="border-radius:15px"></a></td> -->
            <td width="80%" valign="center" style="padding-left: 3%;">
              <a href="https://visualcomet.xyz" target="_blank">
                <papertitle>VisualCOMET: Reasoning about the Dynamic Context of a Still Image</papertitle>
              </a>
              <br>
              <strong>Jae Sung Park</strong>,
              <a href="https://allenai.org/team/chandrab" target="_blank">Chandra Bhagavatula</a>,
              <a href="https://roozbehm.info/" target="_blank">Roozbeh Mottaghi</a>,
              <a href="https://homes.cs.washington.edu/~ali/" target="_blank">Ali Farhadi</a>,
              <a href="https://homes.cs.washington.edu/~yejin/" target="_blank">Yejin Choi</a>
              <br>
              <em>ECCV</em>, 2020 <strong>(Spotlight)</strong>
              <br>
              <a href="https://visualcomet.xyz" target="_blank">project page</a>
        /
              <a href="https://arxiv.org/abs/2004.10796" target="_blank">arXiv</a>
        /
              <a href="https://github.com/jamespark3922/visual-comet" target="_blank">code</a>
              <!-- <p></p>
              <p>Can machines perform visual commonsense reasoning in time? <br> A new task that predicts person's intent and past and future events in images.</p> -->
            </td>
          </tr>

          <tr>
            <!-- <td width="40%" valign="center" align="center"><img src="images/papers/identity20.png" alt="sym" width="100%" style="border-radius:15px"></a></td> -->
            <td width="80%" valign="center" style="padding-left: 3%;">
              <a href="https://sites.google.com/site/describingmovies/lsmdc-2019" target="_blank">
                <papertitle>Identity Aware Multi-Sentence Video Description</papertitle>
              </a>
              <br>
              <strong>Jae Sung Park</strong>,
              <a href="https://people.eecs.berkeley.edu/~trevor/" target="_blank">Trevor Darrell</a>,
              <a href="https://anna-rohrbach.net/" target="_blank">Anna Rohrbach</a>
              <br>
              <em>ECCV</em>, 2020
              <br>
              <a href="https://sites.google.com/site/describingmovies/lsmdc-2019" target="_blank"> project page </a>
              <!--<a href="https://visualcomet.xyz">project page</a>-->
        /
              <a href="https://arxiv.org/abs/2008.09791" target="_blank">arXiv</a>
              <!-- <p></p>
              <p>Link characters within a set of clips and generate descriptions with locally re-identified people.</p> -->
            </td>
          </tr>

          <tr>
            <!-- <td width="40%" valign="center" align="center"><img src="images/papers/cvpr19.png" alt="sym" width="100%" style="border-radius:15px"></a></td> -->
            <td width="80%" valign="center" style="padding-left: 3%;">
              <a href="https://arxiv.org/abs/1812.05634" target="_blank">
                <papertitle>Adversarial Inference for Multi-Sentence Video Description</papertitle>
              </a>
              <br>
              <strong>Jae Sung Park</strong>,
              <a href="https://rohrbach.vision/" target="_blank">Marcus Rohrbach</a>,
              <a href="https://people.eecs.berkeley.edu/~trevor/" target="_blank">Trevor Darrell</a>,
              <a href="https://anna-rohrbach.net/" target="_blank">Anna Rohrbach</a>
              <br>
              <em>CVPR</em>, 2019 <strong>(Oral)</strong>
              <br>
              <a href="https://arxiv.org/abs/1812.05634" target="_blank">arxiv</a>
              /
              <a href="https://github.com/jamespark3922/adv-inf" target="_blank">code</a>
              <br>
            </td>
          </tr>

        </tbody></table>

        </div> <!-- end of research -->

        <p></p>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tbody><tr><td>
            <heading>Service</heading>
            <ul>
              <li>
                <a href="https://multimodalagents.github.io/" target="_blank">ECCV24 Workshop on Multimodal Agents</a> (Co-organizer)
              </li>
              <li>
                <a href="https://multimodalagentai.github.io/" target="_blank">CVPR24 Tutorial on Generalist Agent AI</a> (Co-organizer)
              </li>
              <li>
                <a href="https://sites.google.com/site/describingmovies" target="_blank">Large Scale Movie Description Challenge 2019/2021</a> (Co-organizer)
              </li>
              <li>
                <a href="https://sites.google.com/site/iccv19clvllsmdc/" target="_blank">ICCV19 3rd Workshop on Closing the Loop between Vision & Language</a> (Co-organizer)
              </li>
            </ul>
          </td></tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tbody><tr><td>
            <heading>Teaching</heading>
            <ul>
              <li>[Winter 2021] <a href="https://courses.cs.washington.edu/courses/csep517/21wi/" target="_blank">CSE P 517: Natural Language Processing</a> (TA)</li>
              <li>[Fall 2019] <a href="https://courses.cs.washington.edu/courses/cse599g1/19au/" target="_blank">CSE 599/G1: Intro to Deep Learning</a> (TA)</li>
            </ul>
          </td></tr>
        </tbody></table>


        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="20%" valign="center" align="center"><a href="https://sites.google.com/site/describingmovies"><img src="images/lsmdc2019.png" alt="sym" width="90%" style="border-radius:15px"></a></td>
            <td width="80%" valign="center" style="padding-left: 3%;">
              <a href="https://sites.google.com/site/describingmovies">ICCV19 3rd Workshop on Closing the Loop between Vision & Language</a>
              <p style="margin:0">Co-organizer</p>
              <br>
              <br>
            </td>
          </tr>

          <tr>
            <td width="20%" valign="center" align="center"><a href="https://sites.google.com/site/describingmovies"><img src="images/lsmdc2019.png" alt="sym" width="90%" style="border-radius:15px"></a></td>
            <td width="80%" valign="center" style="padding-left: 3%;">
              <a href="https://sites.google.com/site/describingmovies">Large Scale Movie Description Challenge 2019</a>
              <p style="margin:0">Co-organizer</p>
              <br>
              <br>
            </td>
          </tr>
        </tbody></table> -->

      </td> <!-- end of the page column -->
    </tr> <!-- end of the page row -->
  </table>
</body>

</html>