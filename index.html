<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!--<script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>-->
  <!--<script>-->
    <!--window.dataLayer = window.dataLayer || [];-->
    <!--function gtag(){dataLayer.push(arguments);}-->
    <!--gtag('js', new Date());-->

    <!--gtag('config', 'UA-7580334-2');-->
  <!--</script>-->

  <title>Jae Sung Park</title>

  <meta name="author" content="Jae Sung Park">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/allenschool.png">
</head>

<body>
  <table style="width:100%;max-width:1100px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jae Sung (James) Park</name>
              </p>
              <p>I am a PhD student at University of Washington in <a href="https://www.cs.washington.edu/"> Computer Science and Engineering </a> advised by <a href="https://homes.cs.washington.edu/~yejin/"> Yejin Choi</a> and <a href="https://homes.cs.washington.edu/~ali/"> Ali Farhadi</a>.
                Previously, I received my B.S. degree in EECS at University of California, Berkeley, where I worked closely with <a href="https://anna-rohrbach.net/"> Anna Rohrbach </a> and <a href="https://people.eecs.berkeley.edu/~trevor/"> Trevor Darrell </a>.
              </p>
              <p>
                I am interested in how machines use <b>visual perception</b> and <b>language understanding to reason about the visual world </b> in a way humans do. 
                Specifically, my research projects have been focused on: 
                <ul>
                  <li>Empowering <b>Visual Commonsense Reasoning</b> of AI models</li>
                  <li><b>Grounding</b> Objects, Concepts, Actions to Images and Videos</li>
                  <li><b>Evaluation</b> of Multimodal Language Models</li>
                </ul>
              </p>
              <p style="text-align:center">
                <a href="mailto:jspark96@cs.washington.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=hD2WqqcAAAAJ&hl=en">Google Scholar</a> &nbsp &nbsp
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/profile/james.jpg"><img style="width:80%;max-width:80%" alt="profile photo" src="images/profile/james.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
          <tbody><tr><td>
            <heading>&nbsp;&nbsp;Experience</heading>
            <ul>
            <li>Internship @ <a href="https://www.microsoft.com/en-us/research/group/deep-learning-group/"> Microsoft Research Deep Learning Team</a> in Spring 2022 - Summer 2023.</li>
            <li>Internship @ <a href="https://www.amazon.science/"> Amazon Alexa AI Team</a> in Winter 2022.</li>
            <li>Internship @ <a href="https://mosaic.allenai.org"> AI2 Mosaic Team </a> in Winter 2020.</li>
            <li>Co-organized <a href="https://sites.google.com/site/describingmovies/lsmdc-2019"> Large Scale Movie Description Challenge 2019 </a> appeared in <a href="https://sites.google.com/site/iccv19clvllsmdc/program"> ICCV19 CLVL workshop</a>. </li>
            <li>Started PhD at University of Washington in Fall 2019 - Present.</li>
            </ul>
          </td></tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Publications</heading>
            </td>
          </tr>

          <tr>
            <td width="80%" valign="center">
              <a href="https://arxiv.org/pdf/2205.12630.pdf">
                <papertitle>Multimodal knowledge alignment with reinforcement learning</papertitle>
              </a>
              <br>
              Youngjae Yu, Jiwan Chung, Heeseung Yun, Jack Hessel, <strong>Jae Sung Park</strong>, Ximing Lu, Prithviraj Ammanabrolu, Rowan Zellers, Ronan Le Bras, Gunhee Kim, Yejin Choi
              <br>
              <em>CVPR</em>, 2023 <strong></strong>
              <br>
              <a href="https://arxiv.org/pdf/2205.12630.pdf">arXiv</a>
            </td>
          </tr>

          <tr>
            <td width="80%" valign="center">
              <a href="https://aclanthology.org/2022.naacl-main.261.pdf">
                <papertitle>Exposing the limits of video-text models through contrast sets</papertitle>
              </a>
              <br>
              <strong>Jae Sung Park</strong>, Sheng Shen, Ali Farhadi, Trevor Darrell, Yejin Choi, Anna Rohrbach
              <br>
              <em>NAACL (short)</em>, 2022 <strong></strong>
              <br>
              <a href="https://aclanthology.org/2022.naacl-main.261.pdf">arXiv</a> /
              <a href="https://github.com/jamespark3922/video-lang-contrast-set">code</a>
            </td>
          </tr>

          <tr>
            <td width="80%" valign="center">
              <a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/c6d4eb15f1e84a36eff58eca3627c82e-Paper.pdf">
                <papertitle>Merlot: Multimodal neural script knowledge models</papertitle>
              </a>
              <br>
              Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, <strong>Jae Sung Park</strong>, Jize Cao, Ali Farhadi, Yejin Choi
              <br>
              <em>Neurips</em>, 2021 <strong></strong>
              <br>
              <a href="https://arxiv.org/abs/2106.02636">arXiv</a>
            </td>
          </tr>

          <tr>
            <td width="80%" valign="center">
              <a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/c88d8d0a6097754525e02c2246d8d27f-Paper.pdf">
                <papertitle>LLC: Accurate, multi-purpose learnt low-dimensional binary codes</papertitle>
              </a>
              <br>
              Aditya Kusupati, Matthew Wallingford, Vivek Ramanujan, Raghav Somani, <strong>Jae Sung Park</strong>, Krishna Pillutla, Prateek Jain, Sham Kakade, Ali Farhadi
              <br>
              <em>Neurips</em>, 2021 <strong></strong>
              <br>
              <a href="https://arxiv.org/abs/2106.01487">arXiv</a>
            </td>
          </tr>

          <tr>
            <!-- <td width="40%" valign="center" align="center"><img src="images/papers/visualcomet20.jpg" alt="sym" width="100%" style="border-radius:15px"></a></td> -->
            <td width="80%" valign="center">
              <a href="https://arxiv.org/pdf/2010.07526">
                <papertitle>Natural language rationales with full-stack visual reasoning: From pixels to semantic frames to commonsense graphs</papertitle>
              </a>
              <br>
              Ana MarasoviÄ‡, Chandra Bhagavatula, <strong>Jae Sung Park</strong>, Ronan Le Bras, Noah A Smith, Yejin Choi
              <br>
              <em>Findings of EMNLP</em>, 2020 <strong></strong>
              <br>
              <a href="https://arxiv.org/pdf/2010.07526">arXiv</a>
            </td>
          </tr>

          <tr>
            <!-- <td width="40%" valign="center" align="center"><img src="images/papers/visualcomet20.jpg" alt="sym" width="100%" style="border-radius:15px"></a></td> -->
            <td width="80%" valign="center">
              <a href="https://visualcomet.xyz">
                <papertitle>VisualCOMET: Reasoning about the Dynamic Context of a Still Image</papertitle>
              </a>
              <br>
              <strong>Jae Sung Park</strong>,
              <a href="https://allenai.org/team/chandrab">Chandra Bhagavatula</a>,
              <a href="https://roozbehm.info/">Roozbeh Mottaghi</a>,
              <a href="https://homes.cs.washington.edu/~ali/">Ali Farhadi</a>,
              <a href="https://homes.cs.washington.edu/~yejin/">Yejin Choi</a>
              <br>
              <em>ECCV</em>, 2020 <strong>(Spotlight)</strong>
              <br>
              <a href="https://visualcomet.xyz">project page</a>
        /
              <a href="https://arxiv.org/abs/2004.10796">arXiv</a>
        /
              <a href="https://github.com/jamespark3922/visual-comet">code</a>
              <!-- <p></p>
              <p>Can machines perform visual commonsense reasoning in time? <br> A new task that predicts person's intent and past and future events in images.</p> -->
            </td>
          </tr>

          <tr>
            <!-- <td width="40%" valign="center" align="center"><img src="images/papers/identity20.png" alt="sym" width="100%" style="border-radius:15px"></a></td> -->
            <td width="80%" valign="center">
              <a href="https://sites.google.com/site/describingmovies/lsmdc-2019">
                <papertitle>Identity Aware Multi-Sentence Video Description</papertitle>
              </a>
              <br>
              <strong>Jae Sung Park</strong>,
              <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
              <a href="https://anna-rohrbach.net/">Anna Rohrbach</a>
              <br>
              <em>ECCV</em>, 2020
              <br>
              <a href="https://sites.google.com/site/describingmovies/lsmdc-2019"> project page </a>
              <!--<a href="https://visualcomet.xyz">project page</a>-->
        /
              <a href="https://arxiv.org/abs/2008.09791">arXiv</a>
              <!-- <p></p>
              <p>Link characters within a set of clips and generate descriptions with locally re-identified people.</p> -->
            </td>
          </tr>

          <tr>
            <!-- <td width="40%" valign="center" align="center"><img src="images/papers/cvpr19.png" alt="sym" width="100%" style="border-radius:15px"></a></td> -->
            <td width="80%" valign="center">
              <a href="https://arxiv.org/abs/1812.05634">
                <papertitle>Adversarial Inference for Multi-Sentence Video Description</papertitle>
              </a>
              <br>
              <strong>Jae Sung Park</strong>,
              <a href="https://rohrbach.vision/">Marcus Rohrbach</a>,
              <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
              <a href="https://anna-rohrbach.net/">Anna Rohrbach</a>
              <br>
              <em>CVPR</em>, 2019 <strong>(Oral)</strong>
              <br>
              <a href="https://arxiv.org/abs/1812.05634">arxiv</a>
              /
              <a href="https://github.com/jamespark3922/adv-inf">code</a>
              <br>
            </td>
          </tr>

        </tbody></table>

        <p></p>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td width="20%" valign="center" align="center"><a href="https://sites.google.com/site/describingmovies/lsmdc-2019"><img src="images/lsmdc2019.png" alt="sym" width="90%" style="border-radius:15px"></a></td>
            <td width="80%" valign="center">
              <a href="https://sites.google.com/site/describingmovies/lsmdc-2019">Large Scale Movie Description Challenge 2019</a>
              <p style="margin:0">Co-organizer</p>
              <br>
              <br>
            </td>
          </tr>

          <tr>
              <td width="20%" valign="center" align="center"><a href="https://courses.cs.washington.edu/courses/cse599g1/19au/"><img src="images/cs599.png" alt="sym" width="90%" style="border-radius:15px"></a></td>
              <td width="80%" valign="center">
                <a href="https://courses.cs.washington.edu/courses/cse599g1/19au/">CSE 599/G1: Intro to Deep Learning, Fall 2019</a>
                <p style="margin:0">Teaching Assistant</p>
                <br>
                <br>
              </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small">
                Website template from <a href="https://www.cs.berkeley.edu/~barron/" style="font-size:small">here</a> and <a href="https://www.cs.berkeley.edu/~pathak/" style="font-size:small">here</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>